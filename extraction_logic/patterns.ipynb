{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "335dc5a8-8a7b-4e46-b905-5d9c286f3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Main Title: No title found\n",
      "Extractd Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.cnn.com\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "response = requests.get(url, headers=headers)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "article = soup.find('article') or soup.find('div', class_=re.compile(r'article|content|story', re.I))\n",
    "if article:\n",
    "    title_tag = article.find(['h1', 'h2'])\n",
    "    main_title = title_tag.text.strip() if title_tag else \"No title found\"\n",
    "else:\n",
    "    headings = soup.find_all(['h1', 'h2'])\n",
    "    title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "    main_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Main Title:\", main_title)\n",
    "# headings = soup.find_all(['h1', 'h2'])\n",
    "# title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "# longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "# print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "body_candidates = []\n",
    "if article:\n",
    "    paragraphs = article.find_all('p')\n",
    "    body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100 and not re.search(r'©|All Rights Reserved', p.text, re.I)]\n",
    "else:\n",
    "    paragraphs = soup.find_all('p')\n",
    "    body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100 and not re.search(r'©|All Rights Reserved', p.text, re.I)]\n",
    "article_body = ' '.join(body_candidates[:5]) if body_candidates else \"No body found\"\n",
    "print(\"Extractd Body Snippet:\", article_body[:200] + \"...\" if article_body else article_body)\n",
    "# paragraphs = soup.find_all('p')\n",
    "# body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "# article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "# print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad534e0-7b35-4aad-80c3-6941fd54891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Top stories\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.rt.com\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62574189-1bda-47c8-9840-c51dbc01c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Новости\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.rbc.ru/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d351a67-5c81-4634-8ce0-f8409fa52ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: يتصدر الآن\n",
      "Extracted Body Snippet: أفادت وكالة الأنباء القطرية بأن الدوحة ستستضيف قمة عربية إسلامية طارئة يومي الأحد والاثنين المقبلين- لبحث الهجوم الإسرائيلي على عاصمة قطر. أفادت وكالة الأنباء القطرية بأن الدوحة ستستضيف قمة عربية إسلا...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.aljazeera.net/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7900f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: County Championship day four - Notts hold nerve for win, Leics seek promotion, radio & text\n",
      "Extracted Body Snippet: Witnesses describe screaming and running to flee the scene after hearing the fatal gunshot that killed Kirk. Charlie Kirk's killing is another episode of gun violence in America and the latest in a li...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bbc.com/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d46c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Daily Puzzle Answers\n",
      "Extracted Body Snippet: Our shopping experts unearth the best tech and home essential deals every day. If you make a purchase using our links, CNET may earn a commission. From gaming and the camera to new AI skills and the b...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.cnet.com/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
