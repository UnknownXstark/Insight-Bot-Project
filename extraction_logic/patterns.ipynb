{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "335dc5a8-8a7b-4e46-b905-5d9c286f3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Manhunt continues for Charlie Kirk's killer as rifle believed to be used in shooting found\n",
      "Extracted Body Snippet: © 2025 Cable News Network. A Warner Bros. Discovery Company. All Rights Reserved.  CNN Sans ™ & © 2016 Cable News Network....\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.cnn.com\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ad534e0-7b35-4aad-80c3-6941fd54891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Top stories\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.rt.com\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62574189-1bda-47c8-9840-c51dbc01c074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Новости\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.rbc.ru/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d351a67-5c81-4634-8ce0-f8409fa52ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: يتصدر الآن\n",
      "Extracted Body Snippet: أفادت وكالة الأنباء القطرية بأن الدوحة ستستضيف قمة عربية إسلامية طارئة يومي الأحد والاثنين المقبلين- لبحث الهجوم الإسرائيلي على عاصمة قطر. أفادت وكالة الأنباء القطرية بأن الدوحة ستستضيف قمة عربية إسلا...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.aljazeera.net/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7900f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: County Championship day four - Notts hold nerve for win, Leics seek promotion, radio & text\n",
      "Extracted Body Snippet: Witnesses describe screaming and running to flee the scene after hearing the fatal gunshot that killed Kirk. Charlie Kirk's killing is another episode of gun violence in America and the latest in a li...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bbc.com/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91d46c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Daily Puzzle Answers\n",
      "Extracted Body Snippet: Our shopping experts unearth the best tech and home essential deals every day. If you make a purchase using our links, CNET may earn a commission. From gaming and the camera to new AI skills and the b...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.cnet.com/\"\n",
    "response = requests.get(url)\n",
    "response.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "headings = soup.find_all(['h1', 'h2'])\n",
    "title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "paragraphs = soup.find_all('p')\n",
    "body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "article_body = ' '.join(body_candidates) if body_candidates else \"No body found\"\n",
    "print(\"Extracted Body Snippet:\", article_body[:200] + \"...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
