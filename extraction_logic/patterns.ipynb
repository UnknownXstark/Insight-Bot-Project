{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "335dc5a8-8a7b-4e46-b905-5d9c286f3e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: AudioPodcasts and narrated articles\n",
      "Extracted Body Snippet: New recipes, easy dinner ideas and smart kitchen tips from Melissa Clark, Sam Sifton and our New York Times Cooking editors. New recipes, easy dinner ideas and smart kitchen tips from Melissa Clark, S...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.nytimes.com/\" \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    headings = soup.find_all(['h1', 'h2'])\n",
    "    title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "    longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "    print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "    paragraphs = soup.find_all('p')\n",
    "    body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "    article_body = ' '.join(body_candidates[:5]) if body_candidates else \"No body found\"  # Limit to 5 paragraphs\n",
    "    print(\"Extracted Body Snippet:\", article_body[:200] + \"...\" if article_body else article_body)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(\"Error fetching URL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4e02c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: More climate crisis & environment\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/\" \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    headings = soup.find_all(['h1', 'h2'])\n",
    "    title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "    longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "    print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "    paragraphs = soup.find_all('p')\n",
    "    body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "    article_body = ' '.join(body_candidates[:5]) if body_candidates else \"No body found\"  # Limit to 5 paragraphs\n",
    "    print(\"Extracted Body Snippet:\", article_body[:200] + \"...\" if article_body else article_body)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(\"Error fetching URL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "594441cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Trump vows to impose sanctions on Russia once NATO countries stop buying its oil\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://tass.com/\" \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    headings = soup.find_all(['h1', 'h2'])\n",
    "    title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "    longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "    print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "    paragraphs = soup.find_all('p')\n",
    "    body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "    article_body = ' '.join(body_candidates[:5]) if body_candidates else \"No body found\"  # Limit to 5 paragraphs\n",
    "    print(\"Extracted Body Snippet:\", article_body[:200] + \"...\" if article_body else article_body)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(\"Error fetching URL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05cbebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: Новости\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.rbc.ru/\" \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    headings = soup.find_all(['h1', 'h2'])\n",
    "    title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "    longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "    print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "    paragraphs = soup.find_all('p')\n",
    "    body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "    article_body = ' '.join(body_candidates[:5]) if body_candidates else \"No body found\"  # Limit to 5 paragraphs\n",
    "    print(\"Extracted Body Snippet:\", article_body[:200] + \"...\" if article_body else article_body)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(\"Error fetching URL:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ce7a9b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Title: MB: Ar: SideMenu\n",
      "Extracted Body Snippet: No body found...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://middle-east-online.com/\" \n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "try:\n",
    "    response = requests.get(url, headers=headers, timeout=10)\n",
    "    response.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "\n",
    "    headings = soup.find_all(['h1', 'h2'])\n",
    "    title_candidates = [(h.text.strip(), len(h.text.strip())) for h in headings if h.text.strip()]\n",
    "    longest_title = max(title_candidates, key=lambda x: x[1])[0] if title_candidates else \"No title found\"\n",
    "    print(\"Extracted Title:\", longest_title)\n",
    "\n",
    "    paragraphs = soup.find_all('p')\n",
    "    body_candidates = [p.text.strip() for p in paragraphs if len(p.text.strip()) > 100]\n",
    "    article_body = ' '.join(body_candidates[:5]) if body_candidates else \"No body found\"  # Limit to 5 paragraphs\n",
    "    print(\"Extracted Body Snippet:\", article_body[:200] + \"...\" if article_body else article_body)\n",
    "\n",
    "except requests.RequestException as e:\n",
    "    print(\"Error fetching URL:\", e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
